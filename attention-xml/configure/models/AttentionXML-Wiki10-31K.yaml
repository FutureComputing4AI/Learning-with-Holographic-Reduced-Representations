name: AttentionXML

model:
  hidden_size: 512
  layers_num: 1
  linear_size: [1024, 1024]
  dropout: 0.5
  emb_trainable: False
  spn_dim: False
  no_grad: False
  without_negative: False

train:
  batch_size: 16
  nb_epoch: 30
  swa_warmup: 4

valid:
  batch_size: 32

predict:
  batch_size: 40

path: models
